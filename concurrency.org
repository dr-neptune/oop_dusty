#+TITLE: Concurrency

This chapter looks at:
- Threads
- Multiprocessing
- Futures
- AsyncIO

* Threads

#+BEGIN_SRC python :tangle threading_fun.py
from threading import Thread

class InputReader(Thread):
    def run(self):
        self.line_of_text = input()

print("Enter some text and press enter: ")
thread = InputReader()
thread.start()

count = result = 1
while thread.is_alive():
    result = count * count
    count += 1

print(f"calculated squares up to {count} * {count} = {result}")
print(f"while you typed {thread.line_of_text}")
#+END_SRC

#+BEGIN_SRC python :tangle thread_weather.py
from threading import Thread
import time
from urllib.request import urlopen
from xml.etree import ElementTree

CITIES = {
    "Charlottetown": ("PE", "s0000583"),
    "Edmonton": ("AB", "s0000045"),
    "Fredericton": ("NB", "s0000250"),
    "Halifax": ("NS", "s0000318"),
    "Iqaluit": ("NU", "s0000394"),
    "Qu√©bec City": ("QC", "s0000620"),
    "Regina": ("SK", "s0000788"),
    "St. John's": ("NL", "s0000280"),
    "Toronto": ("ON", "s0000458"),
    "Victoria": ("BC", "s0000775"),
    "Whitehorse": ("YT", "s0000825"),
    "Winnipeg": ("MB", "s0000193"),
    "Yellowknife": ("NT", "s0000366")
}

class TempGetter(Thread):
    def __init__(self, city):
        super().__init__()
        self.city = city
        self.province, self.code = CITIES[self.city]

    def run(self):
        url = (
            "http://dd.weatheroffice.ec.gc.ca/citypage_weather/xml/"
            f"{self.province}/{self.code}_e.xml"
        )
        with urlopen(url) as stream:
            xml = ElementTree.parse(stream)
            self.temperature = xml.find(
                "currentConditions/temperature"
            ).text

threads = [TempGetter(c) for c in CITIES]
start = time.time()
for thread in threads:
    thread.start()
for thread in threads:
    thread.join()
for thread in threads:
    print(f"it is {thread.temperature} degrees C in {thread.city}")

print(f"Got {len(threads)} temps in {time.time() - start} seconds")
#+END_SRC

* Multiprocessing

The multithreading module spins up a new OS process to do the work.

This means that there is an entirely separate Python interpreter running for each process.

#+BEGIN_SRC python :tangle multi_p.py
from multiprocessing import Process, cpu_count
from threading import Thread
import time
import os

class MuchCPU(Thread):
    def run(self):
        print(os.getpid())
        for i in range(200000000):
            pass

if __name__ == '__main__':
    procs = [MuchCPU() for f in range(cpu_count())]
    t = time.time()
    for p in procs:
        p.start()
    for p in procs:
        p.join()
    print(f"work took {time.time() - t} seconds")
#+END_SRC

* Multiprocessing Pools

It might make sense to create at most cpu_count() processes when the program starts and then have them execute tasks as needed.
This has much less overhead than starting a new process for each task.

#+BEGIN_SRC python :tangle pool_mp.py
import random
from multiprocessing import Pool

def prime_factor(value):
    factors = []
    for divisor in range(2, value - 1):
        quotient, remainder = divmod(value, divisor)
        if not remainder:
            factors.extend(prime_factor(divisor))
            factors.extend(prime_factor(quotient))
            break
    else:
        factors = [value]
    return factors

if __name__ == '__main__':
    pool = Pool()
    to_factor = [random.randint(100000, 50000000) for i in range(100)]
    results = pool.map(prime_factor, to_factor)
    for value, factors in zip(to_factor, results):
        print(f"The factors of {value} are {factors}")
#+END_SRC

* Queues

If you need more control over communication between processes, we can use a queue.

To illustrate queues, let's build a little search engine for text content that stores all relevant entries in memory.
This search engine scans all files in the current directory in parallel. A process is constructed for each core in the CPU.
Each of these is instructed to load some of the files into memory.

#+BEGIN_SRC python :tangle mp_queue.py
def search(paths, query_q, results_q):
    """
    Do the file loading and searching
    """
    lines = []
    for path in paths:
        lines.extend(l.strip() for l in path.open())
    query = query_q.get()
    while query:
        results_q.put([l for l in lines if query in l])
        query = query_q.get()


if __name__ == '__main__':
    from multiprocessing import Process, Queue, cpu_count
    from path import Path
    cpus = cpu_count()
    pathnames = [f for f in Path(".").listdir() if f.isfile()]
    paths = [pathnames[i::cpus] for i in range(cpus)]
    query_queues = [Queue() for p in range(cpus)]
    results_queue = Queue()
    search_procs = [
        Process(target=search, args=(p, q, results_queue))
        for p, q in zip(paths, query_queues)
    ]
    for proc in search_procs:
        proc.start()

    for q in query_queues:
        q.put("def")
        q.put(None)  # signal process termination

    for i in range(cpus):
        for match in results_queue.get():
            print(match)
    for proc in search_procs:
        proc.join()

#+END_SRC

* Futures

Futures wrap either multiprocessing or threading depending on what kind of concurrency we need (tending toward I/O vs. tending toward CPU).


A future is an object that wraps a function call. That function call is run in the background in a thread or process.
The future object has methods the main thread can use to check whether the future has completed and to get the results after it has been completed.

We will implement a simple version of the find command.
The example will search the entire filesystem for paths that contain a given string of six characters, as follows:

#+BEGIN_SRC python :tangle py_futures.py
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from os.path import sep as pathsep
from collections import deque

def find_files(path, query_string):
    subdirs = []
    for p in path.iterdir():
        full_path = str(p.absolute())
        if p.is_dir() and not p.is_symlink():
            subdirs.append(p)
        if query_string in full_path:
            print(full_path)
    return subdirs

query = ".py"
futures = deque()
basedir = Path(pathsep).absolute()

with ThreadPoolExecutor(max_workers=10) as executor:
    futures.append(executor.submit(find_files, basedir, query))
    while futures:
        future = futures.popleft()
        if future.exception():
            continue
        elif future.done():
            subdirs = future.result()
            for subdir in subdirs:
                futures.append(executor.submit(find_files, subdir, query))
        else:
            futures.append(future)
#+END_SRC

* AsyncIO

AsyncIO combines the concept of futures and an event loop with coroutines.
It was mostly designed for network I/O. Most networking applications, especially on the server side,
spend a lot of time waiting for data to come in from the network. This can be solved by handling each client in a separate thread,
but threads use up memory and other resources. AsyncIO uses coroutines as a sort of lightweight thread.

The library provides its own event loop, obviating the need for using a while loop like above.
When we run code in an async task on the event loop, that code must return immediately, blocking neither on I/O nor on longrunning calculations.

* AsyncIO in action

A canonical example of a blocking function is the time.sleep call.

#+BEGIN_SRC python
import asyncio
import random

async def random_sleep(counter):
    delay = random.random() * 5
    print(f"{counter} sleeps for {delay:.2f}")
    await asyncio.sleep(delay)
    print(f"{counter} awakens")


async def five_sleepers():
    print("Creating 5 tasks")
    tasks = [asyncio.create_task(random_sleep(i)) for i in range(5)]
    print("Sleeping after starting 5 tasks")
    await asyncio.sleep(2)
    print("Waking and waiting for 5 tasks")
    await asyncio.gather(*tasks)


asyncio.get_event_loop().run_until_complete(five_sleepers())
print("done five tasks")
#+END_SRC

* AsyncIO for Networking

AsyncIO was specifically designed for use with network sockets, so let's implement a DNS server.

#+BEGIN_SRC python
import asyncio
from contextlib import suppress

ip_map = {
    b"facebook.com.": "173.252.120.6",
    b"yougov.com.": "213.52.133.246",
    b"wipo.int.": "193.5.93.80",
    b"dataquest.io.": "104.20.20.199"
}


def lookup_dns(data):
    domain = b""
    pointer, part_length = 13, data[12]
    while part_length:
        domain += data[pointer:pointer + part_length] + b"."
        pointer += part_length + 1
        part_length = data[pointer - 1]
    ip = ip_map.get(domain, "127.0.0.1")
    return domain, ip


def create_response(data, ip):
    ba = bytearray
    packet = ba(data[:2]) + ba([129, 128]) + data[4:6] * 2
    packet += ba(4) + data[12:]
    packet += ba([192, 12, 0, 1, 0, 1, 0, 0, 0, 60, 0, 4])
    for x in ip.split("."):
        packet.append(int(x))
    return packet


class DNSProtocol(asyncio.DatagramProtocol):
    def connection_made(self, transport):
        self.transport = transport

    def datagram_received(self, data, addr):
        print(f"Received request from {addr[0]}")
        domain, ip = lookup_dns(data)
        print(f"Sending IP {domain.decode()} for {ip} to {addr[0]}")
        self.transport.sendto(create_response(data, ip), addr)

loop = asyncio.get_event_loop()
transport, protocol = loop.run_until_complete(
    loop.create_datagram_endpoint(
        DNSProtocol, local_addr = ("127.0.0.1", 4343)
    )
)
print("DNS Server Running")

with suppress(KeyboardInterrupt):
    loop.run_forever()
transport.close()
loop.close()
#+END_SRC

* Using Executors to Wrap Blocking Code

AsyncIO provides its own version of the futures library to allow us to run code in a separate thread or process when there isn't an appropriate non-blocking call to be made.
This allows us to combine threads and processes with the asynchronous model. This is the best of both worlds when an application has bursts of I/O-bound and CPU-bound activity. The IO bound portions can happen in an event loop, while the CPU intensive work can be spun off to a different process.

Let's implement sorting as a service using AsyncIO:

#+BEGIN_SRC python :tangle saas.py
import asyncio
import json
from concurrent.futures import ProcessPoolExecutor


def sort_in_process(data):
    """gnome sort"""
    nums = json.loads(data.decode())
    curr = 1
    while curr < len(nums):
        if nums[curr] >= nums[curr - 1]:
            curr += 1
        else:
            nums[curr], nums[curr - 1] = nums[curr - 1], nums[curr]
            if curr > 1:
                curr -= 1
    return json.dumps(nums).encode()


async def sort_request(reader, writer):
    print("Received Connection")
    length = await reader.read(8)
    data = await reader.readexactly(int.from_bytes(length, "big"))
    result = await asyncio.get_event_loop().run_in_executor(None, sort_in_process, data)
    print("Sorted list")
    writer.write(result)
    writer.close()
    print("Connection Closed")


loop = asyncio.get_event_loop()
loop.set_default_executor(ProcessPoolExecutor())
server = loop.run_until_complete(asyncio.start_server(sort_request, "127.0.0.1", 2015))
print("Sort Service Running")

loop.run_forever()
server.close()
loop.run_until_complete(server.wait_closed())
loop.close()
#+END_SRC

* AsyncIO Clients

AsyncIO is very common for implementing servers, but it is a generic networking library and provides full support for client processes as well.
Clients can be much simpler than servers, as they don't have to be set up to wait for incoming connections.

#+BEGIN_SRC python :tangle asio_client.py
import asyncio
import random
import json

async def remote_sort():
    reader, writer = await asyncio.open_connection("127.0.0.1", 2015)
    print("Generating random list")
    numbers = [random.randrange(10000) for r in range(10000)]
    data = json.dumps(numbers).encode()
    print("List generated, sending data")
    writer.write(len(data).to_bytes(8, "big"))
    writer.write(data)
    print("Waiting for data")
    data = await reader.readexactly(len(data))
    print("Received data")
    sorted_values = json.loads(data.decode())
    print(sorted_values, "\n")
    writer.close()

loop = asyncio.get_event_loop()
loop.run_until_complete(remote_sort())
loop.close()
#+END_SRC

* Case Study

We will build a basic image compression tool. It will take black and white images and attempt to compress it using a very basic form of compression known as run-length encoding.

Run-length encoding takes a sequence of bits and replaces any strings of repeated bits with the number of bits that are repeated. For example, the string 000011000 might be replaced with 04 12 03.

Before we start designing a concurrent system to build such compressed images, we should ask a fundamental question: Is this application I/O bound or CPU-bound?

#+BEGIN_SRC python :tangle compress_image.py
from bitarray import bitarray
from PIL import Image
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path
import sys


def compress_chunk(chunk: bitarray) -> bytearray:
    """
    start with code that compresses a 127-bit chunk using run-length encoding
    """
    print("Compressing chunk")
    compressed = bytearray()
    count = 1
    last = chunk[0]
    for bit in chunk[1:]:
        if bit != last:
            compressed.append(count | (128 * last))
            count = 0
            last = bit
        count += 1
    compressed.append(count | (128 * last))
    return compressed


def compress_row(row: bitarray) -> bytearray:
    """
    compress a row of image data
    """
    print("Compressing row")
    compressed = bytearray()
    chunks = split_bits(row, 127)
    for chunk in chunks:
        compressed.extend(compress_chunk(chunk))
    return compressed


def split_bits(bits, width):
    """generator that splits bits into width length chunks"""
    print("splitting bits")
    for i in range(0, len(bits), width):
        yield bits[i:i+width]


def compress_in_executor(executor, bits, width):
    """
    run everything in a provided executor
    """
    print("Splitting into processes")
    row_compressors = []
    for row in split_bits(bits, width):
        compressor = executor.submit(compress_row, row)
        row_compressors.append(compressor)

    compressed = bytearray()
    for compressor in row_compressors:
        compressed.extend(compressor.result())
    return compressed


def compress_image(in_filename, out_filename, executor=None):
    """
    loads an image file, converts it to bits, and compresses it
    """
    print("Compressing image")
    executor = executor if executor else ProcessPoolExecutor()
    with Image.open(in_filename) as image:
        bits = bitarray(image.convert('1').getdata())
        width, height = image.size

    compressed = compress_in_executor(executor, bits, width)

    with open(out_filename, 'wb') as file:
        file.write(width.to_bytes(2, 'little'))
        file.write(height.to_bytes(2, 'little'))
        file.write(compressed)


def single_image_main():
    print("runnin' and gunnin'")
    in_filename, out_filename = sys.argv[1:3]
    #executor = ThreadPoolExecutor(4)
    executor = ProcessPoolExecutor()
    compress_image(in_filename, out_filename, executor)


# extend to compress all bitmaps in a directory in parallel
# this is not good code, since it splits and then splits again
def compress_dir(in_dir, out_dir):
    if not out_dir.exists():
        out_dir.mkdir()

    executor = ProcessPoolExecutor()
    for file in (f for f in in_dir.iterdir() if f.suffix == '.bmp'):
        out_file = (out_dir / file.name).with_suffix(".rle")
        executor.submit(compress_image, str(file), str(out_file))


def dir_images_main():
    in_dir, out_dir = (Path(p) for p in sys.argv[1:3])
    compress_dir(in_dir, out_dir)

# decompress an image
def decompress(width, height, bytes):
    image = Image.new('1', (width, height))

    col, row = 0, 0
    for byte in bytes:
        color = (byte & 128) >> 7
        count = byte & ~128
        for i in range(count):
            image.putpixel((row, col), color)
            row += 1
        if not row % width:
            col += 1
            row = 0
    return image


# with open(sys.argv[1], 'rb') as file:
#     width = int.from_bytes(file.read(2), 'little')
#     height = int.from_bytes(file.read(2), 'little')
#     image = decompress(width, height, file.read())
#     image.save(sys.argv[2], 'bmp')


if __name__ == '__main__':
    single_image_main()
#+END_SRC
